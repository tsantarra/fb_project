Outline of system architecture:
    Data is input via sources (either streams or static files)
    Sources are fed to feature evaluators, which return weights on what each thinks is important
    The main level system aggregates weights and selects a stream as the primary data source

===================================================================================

Inputs                          Outputs
Files                           Files
Live cams                       Life cams
                                Features

Let receiver decide to drop. Never drop on output.

TODO
    Output file should keep track of time/frames written too.

    Synchronization
        - in live mode (and possibly in file mode too), dragging windows causes desync
        - audio and video files do not line up in file mode. Video lags audio.
            - cough at 1:32 on Closeup2
            - output cough at
                - 1:32 audio
                - 1:46 video
            - file fps = 25

            - found problem: queue.put blocks by default
                - when disabled, video gets ahead of audio now

Halting:
    - current halt criteria is specified as main process window key entry (not output pane, which is separate)
    - need to determine how to appropriately kill subprocess and release devices

Short-term goals:
    - calibration of audio (so sensitive mics don't dominate)

Technical info:
    - audio sample rates: 44100 16000
    - dtypes float32 Int16

Distortion problem:
    - sample rate must be agreed upon by input and output devices



            input_data = [source.read() for source in self._input_sources]
            source_ids = [source.id for source in self._input_sources]

            for input_frames in zip_longest(*input_data):
                self._input_queue.put([input if input is not None else PipelineOutput(source_ids[index], None)
                                       for index, input in enumerate(input_frames)])